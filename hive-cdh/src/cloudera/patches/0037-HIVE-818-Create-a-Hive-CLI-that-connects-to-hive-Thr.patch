From 1efcfb10d9b7222363e03d07690e5d45a511263c Mon Sep 17 00:00:00 2001
From: carl <carl@cloudera.com>
Date: Mon, 4 Jul 2011 23:28:29 -0700
Subject: [PATCH 37/51] HIVE-818 Create a Hive CLI that connects to hive ThriftServer

Reason: New Feature
Author: Ning Zhang
Ref: CDH-3310
---
 build.xml                                          |    6 +-
 .../java/org/apache/hadoop/hive/cli/CliDriver.java |   83 +++-
 .../apache/hadoop/hive/cli/CliSessionState.java    |   62 +++-
 .../apache/hadoop/hive/cli/OptionsProcessor.java   |   25 +-
 .../hadoop/hive/jdbc/HiveDatabaseMetaData.java     |   17 +-
 .../hadoop/hive/jdbc/HiveQueryResultSet.java       |   42 +-
 .../apache/hadoop/hive/jdbc/TestJdbcDriver.java    |    2 +-
 .../hadoop/hive/ql/session/SessionState.java       |   62 ++-
 service/if/hive_service.thrift                     |    2 +
 service/src/gen/thrift/gen-cpp/ThriftHive.cpp      |  204 +++++++++
 service/src/gen/thrift/gen-cpp/ThriftHive.h        |   90 ++++
 .../thrift/gen-cpp/ThriftHive_server.skeleton.cpp  |    5 +
 .../org/apache/hadoop/hive/service/ThriftHive.java |  458 ++++++++++++++++++++
 .../gen/thrift/gen-php/hive_service/ThriftHive.php |  148 +++++++
 .../thrift/gen-py/hive_service/ThriftHive-remote   |    7 +
 .../gen/thrift/gen-py/hive_service/ThriftHive.py   |  120 +++++
 service/src/gen/thrift/gen-rb/thrift_hive.rb       |   51 +++
 .../org/apache/hadoop/hive/service/HiveServer.java |  179 +++++++-
 .../apache/hadoop/hive/service/TestHiveServer.java |   25 +-
 19 files changed, 1500 insertions(+), 88 deletions(-)

diff --git a/build.xml b/build.xml
index cd1e1cb..33772da 100644
--- a/build.xml
+++ b/build.xml
@@ -141,7 +141,7 @@
       <subant target="@{target}">
         <property name="build.dir.hive" location="${build.dir.hive}"/>
         <property name="is-offline" value="${is-offline}"/>
-        <filelist dir="." files="ant/build.xml,shims/build.xml,common/build.xml,serde/build.xml,metastore/build.xml,ql/build.xml,cli/build.xml,contrib/build.xml,service/build.xml,jdbc/build.xml,hwi/build.xml,hbase-handler/build.xml,ant/build.xml"/>
+        <filelist dir="." files="ant/build.xml,shims/build.xml,common/build.xml,serde/build.xml,metastore/build.xml,ql/build.xml,contrib/build.xml,service/build.xml,cli/build.xml,jdbc/build.xml,hwi/build.xml,hbase-handler/build.xml,ant/build.xml"/>
       </subant>
     </sequential>
   </macrodef>
@@ -152,7 +152,7 @@
       <subant target="@{target}">
         <property name="build.dir.hive" location="${build.dir.hive}"/>
         <property name="is-offline" value="${is-offline}"/>
-        <filelist dir="." files="shims/build.xml,common/build.xml,serde/build.xml,metastore/build.xml,ql/build.xml,cli/build.xml,contrib/build.xml,service/build.xml,jdbc/build.xml,hwi/build.xml,hbase-handler/build.xml"/>
+        <filelist dir="." files="shims/build.xml,common/build.xml,serde/build.xml,metastore/build.xml,ql/build.xml,contrib/build.xml,service/build.xml,cli/build.xml,jdbc/build.xml,hwi/build.xml,hbase-handler/build.xml"/>
       </subant>
     </sequential>
   </macrodef>
@@ -224,9 +224,11 @@
   </target>
 
   <target name="test" depends="clean-test,jar-test" description="Run tests">
+  <!-- TODO: temp comment out for testing
     <antcall target="test-shims">
       <param name="hadoop.version.ant-internal" value="${hadoop.security.version}" />
     </antcall>
+    -->
     <for keepgoing="${test.continue.on.failure}" param="file">
       <path>
         <fileset dir="." includes="*/build.xml" excludes="ant/*,odbc/*,shims/*"/>
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
index b4581fe..a7c9ddf 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java
@@ -27,15 +27,14 @@ import java.io.PrintStream;
 import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import jline.Completor;
 import jline.ArgumentCompletor;
-import jline.ArgumentCompletor.ArgumentDelimiter;
 import jline.ArgumentCompletor.AbstractArgumentDelimiter;
+import jline.ArgumentCompletor.ArgumentDelimiter;
+import jline.Completor;
 import jline.ConsoleReader;
 import jline.History;
 import jline.SimpleCompletor;
@@ -45,18 +44,21 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.parse.ParseDriver;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.exec.Utilities.StreamPrinter;
+import org.apache.hadoop.hive.ql.parse.ParseDriver;
 import org.apache.hadoop.hive.ql.processors.CommandProcessor;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorFactory;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.hive.service.HiveClient;
+import org.apache.hadoop.hive.service.HiveServerException;
 import org.apache.hadoop.hive.shims.ShimLoader;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Schema;
+import org.apache.thrift.TException;
 
 /**
  * CliDriver.
@@ -64,8 +66,9 @@ import org.apache.hadoop.hive.metastore.api.Schema;
  */
 public class CliDriver {
 
-  public static final String prompt = "hive";
-  public static final String prompt2 = "    "; // when ';' is not yet seen
+  public static String prompt = "hive";
+  public static String prompt2 = "    "; // when ';' is not yet seen
+  public static final int LINES_TO_FETCH = 40; // number of lines to fetch in batch from remote hive server
 
   public static final String HIVERCFILE = ".hiverc";
 
@@ -80,7 +83,7 @@ public class CliDriver {
   }
 
   public int processCmd(String cmd) {
-    SessionState ss = SessionState.get();
+    CliSessionState ss = (CliSessionState) SessionState.get();
 
     String cmd_trimmed = cmd.trim();
     String[] tokens = cmd_trimmed.split("\\s+");
@@ -149,8 +152,50 @@ public class CliDriver {
           ss.out.println(StringUtils.join(s, "\n"));
         }
       }
+    } else if (ss.isRemoteMode()) { // remote mode -- connecting to remote hive server
+        HiveClient client = ss.getClient();
+        PrintStream out = ss.out;
+        PrintStream err = ss.err;
 
-    } else {
+        try {
+          client.execute(cmd_trimmed);
+          List<String> results;
+          do {
+            results = client.fetchN(LINES_TO_FETCH);
+            for (String line: results) {
+              out.println(line);
+            }
+          } while (results.size() == LINES_TO_FETCH);
+        } catch (HiveServerException e) {
+          ret = e.getErrorCode();
+          if (ret != 0) { // OK if ret == 0 -- reached the EOF
+            String errMsg = e.getMessage();
+            if (errMsg == null) {
+              errMsg = e.toString();
+            }
+            ret = e.getErrorCode();
+            err.println("[Hive Error]: " + errMsg);
+          }
+        } catch (TException e) {
+          String errMsg = e.getMessage();
+          if (errMsg == null) {
+            errMsg = e.toString();
+          }
+          ret = -10002;
+          err.println("[Thrift Error]: " + errMsg);
+        } finally {
+          try {
+            client.clean();
+          } catch (TException e) {
+            String errMsg = e.getMessage();
+            if (errMsg == null) {
+              errMsg = e.toString();
+            }
+            err.println("[Thrift Error]: Hive server is not cleaned due to thrift exception: "
+                + errMsg);
+          }
+        }
+    } else { // local mode
       CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf)conf);
       if (proc != null) {
         if (proc instanceof Driver) {
@@ -168,7 +213,7 @@ public class CliDriver {
           }
 
           ArrayList<String> res = new ArrayList<String>();
-          
+
           if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)) {
             // Print the column names
             boolean first_col = true;
@@ -332,6 +377,7 @@ public class CliDriver {
     // as a keyword delimiter, we need to define a new ArgumentDelimiter
     // that recognizes parenthesis as a delimiter.
     ArgumentDelimiter delim = new AbstractArgumentDelimiter () {
+      @Override
       public boolean isDelimiterChar (String buffer, int pos) {
         char c = buffer.charAt(pos);
         return (Character.isWhitespace(c) || c == '(' || c == ')' ||
@@ -367,7 +413,7 @@ public class CliDriver {
         return ret;
       }
     };
-    
+
     return completor;
   }
 
@@ -416,6 +462,17 @@ public class CliDriver {
 
     SessionState.start(ss);
 
+    // connect to Hive Server
+    if (ss.getHost() != null) {
+      ss.connect();
+      if (ss.isRemoteMode()) {
+        prompt = "[" + ss.host + ':' + ss.port + "] " + prompt;
+        char[] spaces = new char[prompt.length()];
+        Arrays.fill(spaces, ' ');
+        prompt2 = new String(spaces);
+      }
+    }
+
     CliDriver cli = new CliDriver();
 
     // Execute -i init files (always in silent mode)
@@ -463,6 +520,8 @@ public class CliDriver {
       }
     }
 
+    ss.close();
+
     System.exit(ret);
   }
 
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java b/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
index 9075d61..d2267ab 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java
@@ -24,10 +24,17 @@ import java.util.Properties;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.service.HiveClient;
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
 
 /**
  * CliSessionState.
- * 
+ *
  */
 public class CliSessionState extends SessionState {
   /**
@@ -50,12 +57,65 @@ public class CliSessionState extends SessionState {
    */
   public List<String> initFiles = new ArrayList<String>();
 
+  /**
+   * host name and port number of remote Hive server
+   */
+  protected String host;
+  protected int port;
+
+  private boolean remoteMode;
+
+  private TTransport transport;
+  private HiveClient client;
+
   public CliSessionState() {
     super();
+    remoteMode = false;
   }
 
   public CliSessionState(HiveConf conf) {
     super(conf);
+    remoteMode = false;
   }
 
+  /**
+   * Connect to Hive Server
+   */
+  public void connect() throws TTransportException {
+    transport = new TSocket(host, port);
+    TProtocol protocol = new TBinaryProtocol(transport);
+    client = new HiveClient(protocol);
+    transport.open();
+    remoteMode = true;
+  }
+
+  public void setHost(String host) {
+    this.host = host;
+  }
+
+  public String getHost() {
+    return host;
+  }
+
+  public int getPort() {
+    return port;
+  }
+
+  public void close() {
+    try {
+      client.clean();
+      client.shutdown();
+      transport.close();
+    } catch (TException e) {
+      e.printStackTrace();
+    }
+  }
+
+  public boolean isRemoteMode() {
+    return remoteMode;
+  }
+
+  public HiveClient getClient() {
+    return client;
+  }
 }
diff --git a/cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java b/cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java
index e8d46a9..90084ed 100644
--- a/cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java
+++ b/cli/src/java/org/apache/hadoop/hive/cli/OptionsProcessor.java
@@ -73,14 +73,29 @@ public class OptionsProcessor {
         .withDescription("Use value for given property")
         .create());
 
+    // -h hostname/ippaddress
+    options.addOption(OptionBuilder
+        .hasArg()
+        .withArgName("hostname")
+        .withDescription("connecting to Hive Server on remote host")
+        .create('h'));
+
+    // -p port
+    options.addOption(OptionBuilder
+        .hasArg()
+        .withArgName("port")
+        .withDescription("connecting to Hive Server on port number")
+        .create('p'));
+
     // [-S|--silent]
     options.addOption(new Option("S", "silent", false, "Silent mode in interactive shell"));
 
     // [-v|--verbose]
     options.addOption(new Option("v", "verbose", false, "Verbose mode (echo executed SQL to the console)"));
 
-    // [-h|--help]
-    options.addOption(new Option("h", "help", false, "Print help information"));
+    // [-H|--help]
+    options.addOption(new Option("H", "help", false, "Print help information"));
+
   }
 
   public boolean process_stage1(String[] argv) {
@@ -101,7 +116,7 @@ public class OptionsProcessor {
   public boolean process_stage2(CliSessionState ss) {
     ss.getConf();
 
-    if (commandLine.hasOption('h')) {
+    if (commandLine.hasOption('H')) {
       printUsage();
       return false;
     }
@@ -114,6 +129,10 @@ public class OptionsProcessor {
 
     ss.setIsVerbose(commandLine.hasOption('v'));
 
+    ss.host = (String) commandLine.getOptionValue('h');
+
+    ss.port = Integer.parseInt((String) commandLine.getOptionValue('p', "10000"));
+
     String[] initFiles = commandLine.getOptionValues('i');
     if (null != initFiles) {
       ss.initFiles = Arrays.asList(initFiles);
diff --git a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java
index fb7102f..d1ff509 100644
--- a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java
+++ b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveDatabaseMetaData.java
@@ -18,11 +18,6 @@
 
 package org.apache.hadoop.hive.jdbc;
 
-import org.apache.hadoop.hive.metastore.TableType;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Table;
-import org.apache.hadoop.hive.service.HiveInterface;
-
 import java.io.IOException;
 import java.net.URL;
 import java.sql.Connection;
@@ -38,6 +33,12 @@ import java.util.List;
 import java.util.jar.Attributes;
 import java.util.jar.Manifest;
 
+import org.apache.hadoop.hive.metastore.TableType;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.service.HiveInterface;
+import org.apache.thrift.TException;
+
 /**
  * HiveDatabaseMetaData.
  *
@@ -273,7 +274,11 @@ public class HiveDatabaseMetaData implements java.sql.DatabaseMetaData {
   }
 
   public String getDatabaseProductVersion() throws SQLException {
-    return "0";
+    try {
+      return client.getVersion();
+    } catch (TException e) {
+      throw new SQLException(e);
+    }
   }
 
   public int getDefaultTransactionIsolation() throws SQLException {
diff --git a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveQueryResultSet.java b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveQueryResultSet.java
index c80e862..9419ecf 100644
--- a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveQueryResultSet.java
+++ b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveQueryResultSet.java
@@ -30,14 +30,15 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.serde.Constants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.service.HiveInterface;
+import org.apache.hadoop.hive.service.HiveServerException;
 import org.apache.hadoop.io.BytesWritable;
 
 /**
@@ -135,30 +136,33 @@ public class HiveQueryResultSet extends HiveBaseResultSet {
         LOG.debug("Fetched row string: " + rowStr);
       }
 
-      if (!"".equals(rowStr)) {
-        StructObjectInspector soi = (StructObjectInspector) serde.getObjectInspector();
-        List<? extends StructField> fieldRefs = soi.getAllStructFieldRefs();
-        Object data = serde.deserialize(new BytesWritable(rowStr.getBytes()));
-
-        assert row.size() == fieldRefs.size() : row.size() + ", " + fieldRefs.size();
-        for (int i = 0; i < fieldRefs.size(); i++) {
-          StructField fieldRef = fieldRefs.get(i);
-          ObjectInspector oi = fieldRef.getFieldObjectInspector();
-          Object obj = soi.getStructFieldData(data, fieldRef);
-          row.set(i, convertLazyToJava(obj, oi));
-        }
+      StructObjectInspector soi = (StructObjectInspector) serde.getObjectInspector();
+      List<? extends StructField> fieldRefs = soi.getAllStructFieldRefs();
+      Object data = serde.deserialize(new BytesWritable(rowStr.getBytes()));
 
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Deserialized row: " + row);
-        }
+      assert row.size() == fieldRefs.size() : row.size() + ", " + fieldRefs.size();
+      for (int i = 0; i < fieldRefs.size(); i++) {
+        StructField fieldRef = fieldRefs.get(i);
+        ObjectInspector oi = fieldRef.getFieldObjectInspector();
+        Object obj = soi.getStructFieldData(data, fieldRef);
+        row.set(i, convertLazyToJava(obj, oi));
       }
 
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Deserialized row: " + row);
+      }
+    } catch (HiveServerException e) {
+      if (e.getErrorCode() == 0) { // error code == 0 means reached the EOF
+        return false;
+      } else {
+        throw new SQLException("Error retrieving next row", e);
+      }
     } catch (Exception ex) {
       ex.printStackTrace();
-      throw new SQLException("Error retrieving next row");
+      throw new SQLException("Error retrieving next row", ex);
     }
     // NOTE: fetchOne dosn't throw new SQLException("Method not supported").
-    return !"".equals(rowStr);
+    return true;
   }
 
   /**
diff --git a/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java b/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
index bdccf92..b717117 100644
--- a/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
+++ b/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
@@ -707,7 +707,7 @@ public class TestJdbcDriver extends TestCase {
     DatabaseMetaData meta = con.getMetaData();
 
     assertEquals("Hive", meta.getDatabaseProductName());
-    assertEquals("0", meta.getDatabaseProductVersion());
+    assertEquals("1", meta.getDatabaseProductVersion());
     assertEquals(DatabaseMetaData.sqlStateSQL99, meta.getSQLStateType());
     assertNull(meta.getProcedures(null, null, null));
     assertFalse(meta.supportsCatalogsInTableDefinitions());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 65d751e..352404b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -77,6 +77,7 @@ public class SessionState {
    * HiveHistory Object
    */
   protected HiveHistory hiveHist;
+
   /**
    * Streams to read/write from.
    */
@@ -93,16 +94,22 @@ public class SessionState {
   public PrintStream childErr;
 
   /**
+   * Temporary file name used to store results of non-Hive commands (e.g., set, dfs)
+   * and HiveServer.fetch*() function will read results from this file
+   */
+  protected File tmpOutputFile;
+
+  /**
    * type of the command.
    */
   private HiveOperation commandType;
-  
+
   private HiveAuthorizationProvider authorizer;
-  
+
   private HiveAuthenticationProvider authenticator;
-  
+
   private CreateTableAutomaticGrant createTableGrants;
-  
+
   /**
    * Lineage state.
    */
@@ -125,6 +132,14 @@ public class SessionState {
     this.conf = conf;
   }
 
+  public File getTmpOutputFile() {
+    return tmpOutputFile;
+  }
+
+  public void setTmpOutputFile(File f) {
+    tmpOutputFile = f;
+  }
+
   public boolean getIsSilent() {
     if(conf != null) {
       return conf.getBoolVar(HiveConf.ConfVars.HIVESESSIONSILENT);
@@ -182,29 +197,22 @@ public class SessionState {
 
   /**
    * start a new session and set it to current session.
-   * @throws HiveException 
    */
-  public static SessionState start(HiveConf conf) throws HiveException {
+  public static SessionState start(HiveConf conf) {
     SessionState ss = new SessionState(conf);
-    ss.getConf().setVar(HiveConf.ConfVars.HIVESESSIONID, makeSessionId());
-    ss.hiveHist = new HiveHistory(ss);
-    ss.authenticator = HiveUtils.getAuthenticator(conf);
-    ss.authorizer = HiveUtils.getAuthorizeProviderManager(
-        conf, ss.authenticator);
-    ss.createTableGrants = CreateTableAutomaticGrant.create(conf);
-    tss.set(ss);
-    return (ss);
+    return start(ss);
   }
 
   /**
    * set current session to existing session object if a thread is running
    * multiple sessions - it must call this method with the new session object
    * when switching from one session to another.
-   * @throws HiveException 
+   * @throws HiveException
    */
   public static SessionState start(SessionState startSs) {
 
     tss.set(startSs);
+
     if (StringUtils.isEmpty(startSs.getConf().getVar(
         HiveConf.ConfVars.HIVESESSIONID))) {
       startSs.getConf()
@@ -214,7 +222,21 @@ public class SessionState {
     if (startSs.hiveHist == null) {
       startSs.hiveHist = new HiveHistory(startSs);
     }
-    
+
+    if (startSs.getTmpOutputFile() == null) {
+      // per-session temp file containing results to be sent from HiveServer to HiveClient
+      File tmpDir = new File(
+          HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVEHISTORYFILELOC));
+      String sessionID = startSs.getConf().getVar(HiveConf.ConfVars.HIVESESSIONID);
+      try {
+        File tmpFile = File.createTempFile(sessionID, ".pipeout", tmpDir);
+        tmpFile.deleteOnExit();
+        startSs.setTmpOutputFile(tmpFile);
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+
     try {
       startSs.authenticator = HiveUtils.getAuthenticator(startSs
           .getConf());
@@ -225,7 +247,7 @@ public class SessionState {
     } catch (HiveException e) {
       throw new RuntimeException(e);
     }
-    
+
     return startSs;
   }
 
@@ -605,7 +627,7 @@ public class SessionState {
     }
     return commandType.getOperationName();
   }
-  
+
   public HiveOperation getHiveOperation() {
     return commandType;
   }
@@ -613,7 +635,7 @@ public class SessionState {
   public void setCommandType(HiveOperation commandType) {
     this.commandType = commandType;
   }
-  
+
   public HiveAuthorizationProvider getAuthorizer() {
     return authorizer;
   }
@@ -629,7 +651,7 @@ public class SessionState {
   public void setAuthenticator(HiveAuthenticationProvider authenticator) {
     this.authenticator = authenticator;
   }
-  
+
   public CreateTableAutomaticGrant getCreateTableGrants() {
     return createTableGrants;
   }
diff --git a/service/if/hive_service.thrift b/service/if/hive_service.thrift
index 2799010..8f007bc 100644
--- a/service/if/hive_service.thrift
+++ b/service/if/hive_service.thrift
@@ -81,4 +81,6 @@ service ThriftHive extends hive_metastore.ThriftHiveMetastore {
   # Get the queryplan annotated with counter information
   queryplan.QueryPlan getQueryPlan() throws(1:HiveServerException ex)
 
+  # clean up last Hive query (releasing locks etc.)
+  void clean()
 }
diff --git a/service/src/gen/thrift/gen-cpp/ThriftHive.cpp b/service/src/gen/thrift/gen-cpp/ThriftHive.cpp
index 6ecf2ca..c7df1df 100644
--- a/service/src/gen/thrift/gen-cpp/ThriftHive.cpp
+++ b/service/src/gen/thrift/gen-cpp/ThriftHive.cpp
@@ -1391,6 +1391,129 @@ uint32_t ThriftHive_getQueryPlan_presult::read(::apache::thrift::protocol::TProt
   return xfer;
 }
 
+uint32_t ThriftHive_clean_args::read(::apache::thrift::protocol::TProtocol* iprot) {
+
+  uint32_t xfer = 0;
+  std::string fname;
+  ::apache::thrift::protocol::TType ftype;
+  int16_t fid;
+
+  xfer += iprot->readStructBegin(fname);
+
+  using ::apache::thrift::protocol::TProtocolException;
+
+
+  while (true)
+  {
+    xfer += iprot->readFieldBegin(fname, ftype, fid);
+    if (ftype == ::apache::thrift::protocol::T_STOP) {
+      break;
+    }
+    switch (fid)
+    {
+      default:
+        xfer += iprot->skip(ftype);
+        break;
+    }
+    xfer += iprot->readFieldEnd();
+  }
+
+  xfer += iprot->readStructEnd();
+
+  return xfer;
+}
+
+uint32_t ThriftHive_clean_args::write(::apache::thrift::protocol::TProtocol* oprot) const {
+  uint32_t xfer = 0;
+  xfer += oprot->writeStructBegin("ThriftHive_clean_args");
+  xfer += oprot->writeFieldStop();
+  xfer += oprot->writeStructEnd();
+  return xfer;
+}
+
+uint32_t ThriftHive_clean_pargs::write(::apache::thrift::protocol::TProtocol* oprot) const {
+  uint32_t xfer = 0;
+  xfer += oprot->writeStructBegin("ThriftHive_clean_pargs");
+  xfer += oprot->writeFieldStop();
+  xfer += oprot->writeStructEnd();
+  return xfer;
+}
+
+uint32_t ThriftHive_clean_result::read(::apache::thrift::protocol::TProtocol* iprot) {
+
+  uint32_t xfer = 0;
+  std::string fname;
+  ::apache::thrift::protocol::TType ftype;
+  int16_t fid;
+
+  xfer += iprot->readStructBegin(fname);
+
+  using ::apache::thrift::protocol::TProtocolException;
+
+
+  while (true)
+  {
+    xfer += iprot->readFieldBegin(fname, ftype, fid);
+    if (ftype == ::apache::thrift::protocol::T_STOP) {
+      break;
+    }
+    switch (fid)
+    {
+      default:
+        xfer += iprot->skip(ftype);
+        break;
+    }
+    xfer += iprot->readFieldEnd();
+  }
+
+  xfer += iprot->readStructEnd();
+
+  return xfer;
+}
+
+uint32_t ThriftHive_clean_result::write(::apache::thrift::protocol::TProtocol* oprot) const {
+
+  uint32_t xfer = 0;
+
+  xfer += oprot->writeStructBegin("ThriftHive_clean_result");
+
+  xfer += oprot->writeFieldStop();
+  xfer += oprot->writeStructEnd();
+  return xfer;
+}
+
+uint32_t ThriftHive_clean_presult::read(::apache::thrift::protocol::TProtocol* iprot) {
+
+  uint32_t xfer = 0;
+  std::string fname;
+  ::apache::thrift::protocol::TType ftype;
+  int16_t fid;
+
+  xfer += iprot->readStructBegin(fname);
+
+  using ::apache::thrift::protocol::TProtocolException;
+
+
+  while (true)
+  {
+    xfer += iprot->readFieldBegin(fname, ftype, fid);
+    if (ftype == ::apache::thrift::protocol::T_STOP) {
+      break;
+    }
+    switch (fid)
+    {
+      default:
+        xfer += iprot->skip(ftype);
+        break;
+    }
+    xfer += iprot->readFieldEnd();
+  }
+
+  xfer += iprot->readStructEnd();
+
+  return xfer;
+}
+
 void ThriftHiveClient::execute(const std::string& query)
 {
   send_execute(query);
@@ -1884,6 +2007,60 @@ void ThriftHiveClient::recv_getQueryPlan(Apache::Hadoop::Hive::QueryPlan& _retur
   throw ::apache::thrift::TApplicationException(::apache::thrift::TApplicationException::MISSING_RESULT, "getQueryPlan failed: unknown result");
 }
 
+void ThriftHiveClient::clean()
+{
+  send_clean();
+  recv_clean();
+}
+
+void ThriftHiveClient::send_clean()
+{
+  int32_t cseqid = 0;
+  oprot_->writeMessageBegin("clean", ::apache::thrift::protocol::T_CALL, cseqid);
+
+  ThriftHive_clean_pargs args;
+  args.write(oprot_);
+
+  oprot_->writeMessageEnd();
+  oprot_->getTransport()->flush();
+  oprot_->getTransport()->writeEnd();
+}
+
+void ThriftHiveClient::recv_clean()
+{
+
+  int32_t rseqid = 0;
+  std::string fname;
+  ::apache::thrift::protocol::TMessageType mtype;
+
+  iprot_->readMessageBegin(fname, mtype, rseqid);
+  if (mtype == ::apache::thrift::protocol::T_EXCEPTION) {
+    ::apache::thrift::TApplicationException x;
+    x.read(iprot_);
+    iprot_->readMessageEnd();
+    iprot_->getTransport()->readEnd();
+    throw x;
+  }
+  if (mtype != ::apache::thrift::protocol::T_REPLY) {
+    iprot_->skip(::apache::thrift::protocol::T_STRUCT);
+    iprot_->readMessageEnd();
+    iprot_->getTransport()->readEnd();
+    throw ::apache::thrift::TApplicationException(::apache::thrift::TApplicationException::INVALID_MESSAGE_TYPE);
+  }
+  if (fname.compare("clean") != 0) {
+    iprot_->skip(::apache::thrift::protocol::T_STRUCT);
+    iprot_->readMessageEnd();
+    iprot_->getTransport()->readEnd();
+    throw ::apache::thrift::TApplicationException(::apache::thrift::TApplicationException::WRONG_METHOD_NAME);
+  }
+  ThriftHive_clean_presult result;
+  result.read(iprot_);
+  iprot_->readMessageEnd();
+  iprot_->getTransport()->readEnd();
+
+  return;
+}
+
 bool ThriftHiveProcessor::process(boost::shared_ptr< ::apache::thrift::protocol::TProtocol> piprot, boost::shared_ptr< ::apache::thrift::protocol::TProtocol> poprot) {
 
   ::apache::thrift::protocol::TProtocol* iprot = piprot.get();
@@ -2167,5 +2344,32 @@ void ThriftHiveProcessor::process_getQueryPlan(int32_t seqid, ::apache::thrift::
   oprot->getTransport()->writeEnd();
 }
 
+void ThriftHiveProcessor::process_clean(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot)
+{
+  ThriftHive_clean_args args;
+  args.read(iprot);
+  iprot->readMessageEnd();
+  iprot->getTransport()->readEnd();
+
+  ThriftHive_clean_result result;
+  try {
+    iface_->clean();
+  } catch (const std::exception& e) {
+    ::apache::thrift::TApplicationException x(e.what());
+    oprot->writeMessageBegin("clean", ::apache::thrift::protocol::T_EXCEPTION, seqid);
+    x.write(oprot);
+    oprot->writeMessageEnd();
+    oprot->getTransport()->flush();
+    oprot->getTransport()->writeEnd();
+    return;
+  }
+
+  oprot->writeMessageBegin("clean", ::apache::thrift::protocol::T_REPLY, seqid);
+  result.write(oprot);
+  oprot->writeMessageEnd();
+  oprot->getTransport()->flush();
+  oprot->getTransport()->writeEnd();
+}
+
 }}} // namespace
 
diff --git a/service/src/gen/thrift/gen-cpp/ThriftHive.h b/service/src/gen/thrift/gen-cpp/ThriftHive.h
index 70b12d8..e7934fe 100644
--- a/service/src/gen/thrift/gen-cpp/ThriftHive.h
+++ b/service/src/gen/thrift/gen-cpp/ThriftHive.h
@@ -23,6 +23,7 @@ class ThriftHiveIf : virtual public Apache::Hadoop::Hive::ThriftHiveMetastoreIf
   virtual void getThriftSchema(Apache::Hadoop::Hive::Schema& _return) = 0;
   virtual void getClusterStatus(HiveClusterStatus& _return) = 0;
   virtual void getQueryPlan(Apache::Hadoop::Hive::QueryPlan& _return) = 0;
+  virtual void clean() = 0;
 };
 
 class ThriftHiveNull : virtual public ThriftHiveIf , virtual public Apache::Hadoop::Hive::ThriftHiveMetastoreNull {
@@ -52,6 +53,9 @@ class ThriftHiveNull : virtual public ThriftHiveIf , virtual public Apache::Hado
   void getQueryPlan(Apache::Hadoop::Hive::QueryPlan& /* _return */) {
     return;
   }
+  void clean() {
+    return;
+  }
 };
 
 typedef struct _ThriftHive_execute_args__isset {
@@ -836,6 +840,80 @@ class ThriftHive_getQueryPlan_presult {
 
 };
 
+
+class ThriftHive_clean_args {
+ public:
+
+  ThriftHive_clean_args() {
+  }
+
+  virtual ~ThriftHive_clean_args() throw() {}
+
+
+  bool operator == (const ThriftHive_clean_args & /* rhs */) const
+  {
+    return true;
+  }
+  bool operator != (const ThriftHive_clean_args &rhs) const {
+    return !(*this == rhs);
+  }
+
+  bool operator < (const ThriftHive_clean_args & ) const;
+
+  uint32_t read(::apache::thrift::protocol::TProtocol* iprot);
+  uint32_t write(::apache::thrift::protocol::TProtocol* oprot) const;
+
+};
+
+
+class ThriftHive_clean_pargs {
+ public:
+
+
+  virtual ~ThriftHive_clean_pargs() throw() {}
+
+
+  uint32_t write(::apache::thrift::protocol::TProtocol* oprot) const;
+
+};
+
+
+class ThriftHive_clean_result {
+ public:
+
+  ThriftHive_clean_result() {
+  }
+
+  virtual ~ThriftHive_clean_result() throw() {}
+
+
+  bool operator == (const ThriftHive_clean_result & /* rhs */) const
+  {
+    return true;
+  }
+  bool operator != (const ThriftHive_clean_result &rhs) const {
+    return !(*this == rhs);
+  }
+
+  bool operator < (const ThriftHive_clean_result & ) const;
+
+  uint32_t read(::apache::thrift::protocol::TProtocol* iprot);
+  uint32_t write(::apache::thrift::protocol::TProtocol* oprot) const;
+
+};
+
+
+class ThriftHive_clean_presult {
+ public:
+
+
+  virtual ~ThriftHive_clean_presult() throw() {}
+
+
+  uint32_t read(::apache::thrift::protocol::TProtocol* iprot);
+
+};
+
 class ThriftHiveClient : virtual public ThriftHiveIf, public Apache::Hadoop::Hive::ThriftHiveMetastoreClient {
  public:
   ThriftHiveClient(boost::shared_ptr< ::apache::thrift::protocol::TProtocol> prot) :
@@ -872,6 +950,9 @@ class ThriftHiveClient : virtual public ThriftHiveIf, public Apache::Hadoop::Hiv
   void getQueryPlan(Apache::Hadoop::Hive::QueryPlan& _return);
   void send_getQueryPlan();
   void recv_getQueryPlan(Apache::Hadoop::Hive::QueryPlan& _return);
+  void clean();
+  void send_clean();
+  void recv_clean();
 };
 
 class ThriftHiveProcessor : virtual public ::apache::thrift::TProcessor, public Apache::Hadoop::Hive::ThriftHiveMetastoreProcessor {
@@ -888,6 +969,7 @@ class ThriftHiveProcessor : virtual public ::apache::thrift::TProcessor, public
   void process_getThriftSchema(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot);
   void process_getClusterStatus(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot);
   void process_getQueryPlan(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot);
+  void process_clean(int32_t seqid, ::apache::thrift::protocol::TProtocol* iprot, ::apache::thrift::protocol::TProtocol* oprot);
  public:
   ThriftHiveProcessor(boost::shared_ptr<ThriftHiveIf> iface) :
     Apache::Hadoop::Hive::ThriftHiveMetastoreProcessor(iface),
@@ -900,6 +982,7 @@ class ThriftHiveProcessor : virtual public ::apache::thrift::TProcessor, public
     processMap_["getThriftSchema"] = &ThriftHiveProcessor::process_getThriftSchema;
     processMap_["getClusterStatus"] = &ThriftHiveProcessor::process_getClusterStatus;
     processMap_["getQueryPlan"] = &ThriftHiveProcessor::process_getQueryPlan;
+    processMap_["clean"] = &ThriftHiveProcessor::process_clean;
   }
 
   virtual bool process(boost::shared_ptr< ::apache::thrift::protocol::TProtocol> piprot, boost::shared_ptr< ::apache::thrift::protocol::TProtocol> poprot);
@@ -1014,6 +1097,13 @@ class ThriftHiveMultiface : virtual public ThriftHiveIf, public Apache::Hadoop::
     }
   }
 
+  void clean() {
+    uint32_t sz = ifaces_.size();
+    for (uint32_t i = 0; i < sz; ++i) {
+      ifaces_[i]->clean();
+    }
+  }
+
 };
 
 }}} // namespace
diff --git a/service/src/gen/thrift/gen-cpp/ThriftHive_server.skeleton.cpp b/service/src/gen/thrift/gen-cpp/ThriftHive_server.skeleton.cpp
index 7651d92..7ba0ac2 100644
--- a/service/src/gen/thrift/gen-cpp/ThriftHive_server.skeleton.cpp
+++ b/service/src/gen/thrift/gen-cpp/ThriftHive_server.skeleton.cpp
@@ -62,6 +62,11 @@ class ThriftHiveHandler : virtual public ThriftHiveIf {
     printf("getQueryPlan\n");
   }
 
+  void clean() {
+    // Your implementation goes here
+    printf("clean\n");
+  }
+
 };
 
 int main(int argc, char **argv) {
diff --git a/service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java b/service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java
index a21dd8c..c1a2dd8 100644
--- a/service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java
+++ b/service/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java
@@ -46,6 +46,8 @@ public class ThriftHive {
 
     public org.apache.hadoop.hive.ql.plan.api.QueryPlan getQueryPlan() throws HiveServerException, TException;
 
+    public void clean() throws TException;
+
   }
 
   public interface AsyncIface extends org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore .AsyncIface {
@@ -66,6 +68,8 @@ public class ThriftHive {
 
     public void getQueryPlan(AsyncMethodCallback<AsyncClient.getQueryPlan_call> resultHandler) throws TException;
 
+    public void clean(AsyncMethodCallback<AsyncClient.clean_call> resultHandler) throws TException;
+
   }
 
   public static class Client extends org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Client implements TServiceClient, Iface {
@@ -392,6 +396,38 @@ public class ThriftHive {
       throw new TApplicationException(TApplicationException.MISSING_RESULT, "getQueryPlan failed: unknown result");
     }
 
+    public void clean() throws TException
+    {
+      send_clean();
+      recv_clean();
+    }
+
+    public void send_clean() throws TException
+    {
+      oprot_.writeMessageBegin(new TMessage("clean", TMessageType.CALL, ++seqid_));
+      clean_args args = new clean_args();
+      args.write(oprot_);
+      oprot_.writeMessageEnd();
+      oprot_.getTransport().flush();
+    }
+
+    public void recv_clean() throws TException
+    {
+      TMessage msg = iprot_.readMessageBegin();
+      if (msg.type == TMessageType.EXCEPTION) {
+        TApplicationException x = TApplicationException.read(iprot_);
+        iprot_.readMessageEnd();
+        throw x;
+      }
+      if (msg.seqid != seqid_) {
+        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "clean failed: out of sequence response");
+      }
+      clean_result result = new clean_result();
+      result.read(iprot_);
+      iprot_.readMessageEnd();
+      return;
+    }
+
   }
   public static class AsyncClient extends org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.AsyncClient implements AsyncIface {
     public static class Factory implements TAsyncClientFactory<AsyncClient> {
@@ -640,6 +676,34 @@ public class ThriftHive {
       }
     }
 
+    public void clean(AsyncMethodCallback<clean_call> resultHandler) throws TException {
+      checkReady();
+      clean_call method_call = new clean_call(resultHandler, this, protocolFactory, transport);
+      manager.call(method_call);
+    }
+
+    public static class clean_call extends TAsyncMethodCall {
+      public clean_call(AsyncMethodCallback<clean_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
+        super(client, protocolFactory, transport, resultHandler, false);
+      }
+
+      public void write_args(TProtocol prot) throws TException {
+        prot.writeMessageBegin(new TMessage("clean", TMessageType.CALL, 0));
+        clean_args args = new clean_args();
+        args.write(prot);
+        prot.writeMessageEnd();
+      }
+
+      public void getResult() throws TException {
+        if (getState() != State.RESPONSE_READ) {
+          throw new IllegalStateException("Method call not finished!");
+        }
+        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
+        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
+        (new Client(prot)).recv_clean();
+      }
+    }
+
   }
 
   public static class Processor extends org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Processor implements TProcessor {
@@ -656,6 +720,7 @@ public class ThriftHive {
       processMap_.put("getThriftSchema", new getThriftSchema());
       processMap_.put("getClusterStatus", new getClusterStatus());
       processMap_.put("getQueryPlan", new getQueryPlan());
+      processMap_.put("clean", new clean());
     }
 
     private Iface iface_;
@@ -982,6 +1047,32 @@ public class ThriftHive {
 
     }
 
+    private class clean implements ProcessFunction {
+      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
+      {
+        clean_args args = new clean_args();
+        try {
+          args.read(iprot);
+        } catch (TProtocolException e) {
+          iprot.readMessageEnd();
+          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
+          oprot.writeMessageBegin(new TMessage("clean", TMessageType.EXCEPTION, seqid));
+          x.write(oprot);
+          oprot.writeMessageEnd();
+          oprot.getTransport().flush();
+          return;
+        }
+        iprot.readMessageEnd();
+        clean_result result = new clean_result();
+        iface_.clean();
+        oprot.writeMessageBegin(new TMessage("clean", TMessageType.REPLY, seqid));
+        result.write(oprot);
+        oprot.writeMessageEnd();
+        oprot.getTransport().flush();
+      }
+
+    }
+
   }
 
   public static class execute_args implements TBase<execute_args, execute_args._Fields>, java.io.Serializable, Cloneable   {
@@ -5553,4 +5644,371 @@ public class ThriftHive {
 
   }
 
+  public static class clean_args implements TBase<clean_args, clean_args._Fields>, java.io.Serializable, Cloneable   {
+    private static final TStruct STRUCT_DESC = new TStruct("clean_args");
+
+
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public enum _Fields implements TFieldIdEnum {
+;
+
+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();
+
+      static {
+        for (_Fields field : EnumSet.allOf(_Fields.class)) {
+          byName.put(field.getFieldName(), field);
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, or null if its not found.
+       */
+      public static _Fields findByThriftId(int fieldId) {
+        switch(fieldId) {
+          default:
+            return null;
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, throwing an exception
+       * if it is not found.
+       */
+      public static _Fields findByThriftIdOrThrow(int fieldId) {
+        _Fields fields = findByThriftId(fieldId);
+        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
+        return fields;
+      }
+
+      /**
+       * Find the _Fields constant that matches name, or null if its not found.
+       */
+      public static _Fields findByName(String name) {
+        return byName.get(name);
+      }
+
+      private final short _thriftId;
+      private final String _fieldName;
+
+      _Fields(short thriftId, String fieldName) {
+        _thriftId = thriftId;
+        _fieldName = fieldName;
+      }
+
+      public short getThriftFieldId() {
+        return _thriftId;
+      }
+
+      public String getFieldName() {
+        return _fieldName;
+      }
+    }
+    public static final Map<_Fields, FieldMetaData> metaDataMap;
+    static {
+      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
+      metaDataMap = Collections.unmodifiableMap(tmpMap);
+      FieldMetaData.addStructMetaDataMap(clean_args.class, metaDataMap);
+    }
+
+    public clean_args() {
+    }
+
+    /**
+     * Performs a deep copy on <i>other</i>.
+     */
+    public clean_args(clean_args other) {
+    }
+
+    public clean_args deepCopy() {
+      return new clean_args(this);
+    }
+
+    @Override
+    public void clear() {
+    }
+
+    public void setFieldValue(_Fields field, Object value) {
+      switch (field) {
+      }
+    }
+
+    public Object getFieldValue(_Fields field) {
+      switch (field) {
+      }
+      throw new IllegalStateException();
+    }
+
+    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
+    public boolean isSet(_Fields field) {
+      if (field == null) {
+        throw new IllegalArgumentException();
+      }
+
+      switch (field) {
+      }
+      throw new IllegalStateException();
+    }
+
+    @Override
+    public boolean equals(Object that) {
+      if (that == null)
+        return false;
+      if (that instanceof clean_args)
+        return this.equals((clean_args)that);
+      return false;
+    }
+
+    public boolean equals(clean_args that) {
+      if (that == null)
+        return false;
+
+      return true;
+    }
+
+    @Override
+    public int hashCode() {
+      return 0;
+    }
+
+    public int compareTo(clean_args other) {
+      if (!getClass().equals(other.getClass())) {
+        return getClass().getName().compareTo(other.getClass().getName());
+      }
+
+      int lastComparison = 0;
+      clean_args typedOther = (clean_args)other;
+
+      return 0;
+    }
+
+    public _Fields fieldForId(int fieldId) {
+      return _Fields.findByThriftId(fieldId);
+    }
+
+    public void read(TProtocol iprot) throws TException {
+      TField field;
+      iprot.readStructBegin();
+      while (true)
+      {
+        field = iprot.readFieldBegin();
+        if (field.type == TType.STOP) {
+          break;
+        }
+        switch (field.id) {
+          default:
+            TProtocolUtil.skip(iprot, field.type);
+        }
+        iprot.readFieldEnd();
+      }
+      iprot.readStructEnd();
+      validate();
+    }
+
+    public void write(TProtocol oprot) throws TException {
+      validate();
+
+      oprot.writeStructBegin(STRUCT_DESC);
+      oprot.writeFieldStop();
+      oprot.writeStructEnd();
+    }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder("clean_args(");
+      boolean first = true;
+
+      sb.append(")");
+      return sb.toString();
+    }
+
+    public void validate() throws TException {
+      // check for required fields
+    }
+
+  }
+
+  public static class clean_result implements TBase<clean_result, clean_result._Fields>, java.io.Serializable, Cloneable   {
+    private static final TStruct STRUCT_DESC = new TStruct("clean_result");
+
+
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public enum _Fields implements TFieldIdEnum {
+;
+
+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();
+
+      static {
+        for (_Fields field : EnumSet.allOf(_Fields.class)) {
+          byName.put(field.getFieldName(), field);
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, or null if its not found.
+       */
+      public static _Fields findByThriftId(int fieldId) {
+        switch(fieldId) {
+          default:
+            return null;
+        }
+      }
+
+      /**
+       * Find the _Fields constant that matches fieldId, throwing an exception
+       * if it is not found.
+       */
+      public static _Fields findByThriftIdOrThrow(int fieldId) {
+        _Fields fields = findByThriftId(fieldId);
+        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
+        return fields;
+      }
+
+      /**
+       * Find the _Fields constant that matches name, or null if its not found.
+       */
+      public static _Fields findByName(String name) {
+        return byName.get(name);
+      }
+
+      private final short _thriftId;
+      private final String _fieldName;
+
+      _Fields(short thriftId, String fieldName) {
+        _thriftId = thriftId;
+        _fieldName = fieldName;
+      }
+
+      public short getThriftFieldId() {
+        return _thriftId;
+      }
+
+      public String getFieldName() {
+        return _fieldName;
+      }
+    }
+    public static final Map<_Fields, FieldMetaData> metaDataMap;
+    static {
+      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
+      metaDataMap = Collections.unmodifiableMap(tmpMap);
+      FieldMetaData.addStructMetaDataMap(clean_result.class, metaDataMap);
+    }
+
+    public clean_result() {
+    }
+
+    /**
+     * Performs a deep copy on <i>other</i>.
+     */
+    public clean_result(clean_result other) {
+    }
+
+    public clean_result deepCopy() {
+      return new clean_result(this);
+    }
+
+    @Override
+    public void clear() {
+    }
+
+    public void setFieldValue(_Fields field, Object value) {
+      switch (field) {
+      }
+    }
+
+    public Object getFieldValue(_Fields field) {
+      switch (field) {
+      }
+      throw new IllegalStateException();
+    }
+
+    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
+    public boolean isSet(_Fields field) {
+      if (field == null) {
+        throw new IllegalArgumentException();
+      }
+
+      switch (field) {
+      }
+      throw new IllegalStateException();
+    }
+
+    @Override
+    public boolean equals(Object that) {
+      if (that == null)
+        return false;
+      if (that instanceof clean_result)
+        return this.equals((clean_result)that);
+      return false;
+    }
+
+    public boolean equals(clean_result that) {
+      if (that == null)
+        return false;
+
+      return true;
+    }
+
+    @Override
+    public int hashCode() {
+      return 0;
+    }
+
+    public int compareTo(clean_result other) {
+      if (!getClass().equals(other.getClass())) {
+        return getClass().getName().compareTo(other.getClass().getName());
+      }
+
+      int lastComparison = 0;
+      clean_result typedOther = (clean_result)other;
+
+      return 0;
+    }
+
+    public _Fields fieldForId(int fieldId) {
+      return _Fields.findByThriftId(fieldId);
+    }
+
+    public void read(TProtocol iprot) throws TException {
+      TField field;
+      iprot.readStructBegin();
+      while (true)
+      {
+        field = iprot.readFieldBegin();
+        if (field.type == TType.STOP) {
+          break;
+        }
+        switch (field.id) {
+          default:
+            TProtocolUtil.skip(iprot, field.type);
+        }
+        iprot.readFieldEnd();
+      }
+      iprot.readStructEnd();
+      validate();
+    }
+
+    public void write(TProtocol oprot) throws TException {
+      oprot.writeStructBegin(STRUCT_DESC);
+
+      oprot.writeFieldStop();
+      oprot.writeStructEnd();
+    }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder("clean_result(");
+      boolean first = true;
+
+      sb.append(")");
+      return sb.toString();
+    }
+
+    public void validate() throws TException {
+      // check for required fields
+    }
+
+  }
+
 }
diff --git a/service/src/gen/thrift/gen-php/hive_service/ThriftHive.php b/service/src/gen/thrift/gen-php/hive_service/ThriftHive.php
index 48d281a..e01d95d 100644
--- a/service/src/gen/thrift/gen-php/hive_service/ThriftHive.php
+++ b/service/src/gen/thrift/gen-php/hive_service/ThriftHive.php
@@ -18,6 +18,7 @@ interface ThriftHiveIf extends ThriftHiveMetastoreIf {
   public function getThriftSchema();
   public function getClusterStatus();
   public function getQueryPlan();
+  public function clean();
 }
 
 class ThriftHiveClient extends ThriftHiveMetastoreClient implements ThriftHiveIf {
@@ -448,6 +449,53 @@ class ThriftHiveClient extends ThriftHiveMetastoreClient implements ThriftHiveIf
     throw new Exception("getQueryPlan failed: unknown result");
   }
 
+  public function clean()
+  {
+    $this->send_clean();
+    $this->recv_clean();
+  }
+
+  public function send_clean()
+  {
+    $args = new ThriftHive_clean_args();
+    $bin_accel = ($this->output_ instanceof TProtocol::$TBINARYPROTOCOLACCELERATED) && function_exists('thrift_protocol_write_binary');
+    if ($bin_accel)
+    {
+      thrift_protocol_write_binary($this->output_, 'clean', TMessageType::CALL, $args, $this->seqid_, $this->output_->isStrictWrite());
+    }
+    else
+    {
+      $this->output_->writeMessageBegin('clean', TMessageType::CALL, $this->seqid_);
+      $args->write($this->output_);
+      $this->output_->writeMessageEnd();
+      $this->output_->getTransport()->flush();
+    }
+  }
+
+  public function recv_clean()
+  {
+    $bin_accel = ($this->input_ instanceof TProtocol::$TBINARYPROTOCOLACCELERATED) && function_exists('thrift_protocol_read_binary');
+    if ($bin_accel) $result = thrift_protocol_read_binary($this->input_, 'ThriftHive_clean_result', $this->input_->isStrictRead());
+    else
+    {
+      $rseqid = 0;
+      $fname = null;
+      $mtype = 0;
+
+      $this->input_->readMessageBegin($fname, $mtype, $rseqid);
+      if ($mtype == TMessageType::EXCEPTION) {
+        $x = new TApplicationException();
+        $x->read($this->input_);
+        $this->input_->readMessageEnd();
+        throw $x;
+      }
+      $result = new ThriftHive_clean_result();
+      $result->read($this->input_);
+      $this->input_->readMessageEnd();
+    }
+    return;
+  }
+
 }
 
 // HELPER FUNCTIONS AND STRUCTURES
@@ -1700,4 +1748,104 @@ class ThriftHive_getQueryPlan_result {
 
 }
 
+class ThriftHive_clean_args {
+  static $_TSPEC;
+
+
+  public function __construct() {
+    if (!isset(self::$_TSPEC)) {
+      self::$_TSPEC = array(
+        );
+    }
+  }
+
+  public function getName() {
+    return 'ThriftHive_clean_args';
+  }
+
+  public function read($input)
+  {
+    $xfer = 0;
+    $fname = null;
+    $ftype = 0;
+    $fid = 0;
+    $xfer += $input->readStructBegin($fname);
+    while (true)
+    {
+      $xfer += $input->readFieldBegin($fname, $ftype, $fid);
+      if ($ftype == TType::STOP) {
+        break;
+      }
+      switch ($fid)
+      {
+        default:
+          $xfer += $input->skip($ftype);
+          break;
+      }
+      $xfer += $input->readFieldEnd();
+    }
+    $xfer += $input->readStructEnd();
+    return $xfer;
+  }
+
+  public function write($output) {
+    $xfer = 0;
+    $xfer += $output->writeStructBegin('ThriftHive_clean_args');
+    $xfer += $output->writeFieldStop();
+    $xfer += $output->writeStructEnd();
+    return $xfer;
+  }
+
+}
+
+class ThriftHive_clean_result {
+  static $_TSPEC;
+
+
+  public function __construct() {
+    if (!isset(self::$_TSPEC)) {
+      self::$_TSPEC = array(
+        );
+    }
+  }
+
+  public function getName() {
+    return 'ThriftHive_clean_result';
+  }
+
+  public function read($input)
+  {
+    $xfer = 0;
+    $fname = null;
+    $ftype = 0;
+    $fid = 0;
+    $xfer += $input->readStructBegin($fname);
+    while (true)
+    {
+      $xfer += $input->readFieldBegin($fname, $ftype, $fid);
+      if ($ftype == TType::STOP) {
+        break;
+      }
+      switch ($fid)
+      {
+        default:
+          $xfer += $input->skip($ftype);
+          break;
+      }
+      $xfer += $input->readFieldEnd();
+    }
+    $xfer += $input->readStructEnd();
+    return $xfer;
+  }
+
+  public function write($output) {
+    $xfer = 0;
+    $xfer += $output->writeStructBegin('ThriftHive_clean_result');
+    $xfer += $output->writeFieldStop();
+    $xfer += $output->writeStructEnd();
+    return $xfer;
+  }
+
+}
+
 ?>
diff --git a/service/src/gen/thrift/gen-py/hive_service/ThriftHive-remote b/service/src/gen/thrift/gen-py/hive_service/ThriftHive-remote
index b641989..0a36fc0 100644
--- a/service/src/gen/thrift/gen-py/hive_service/ThriftHive-remote
+++ b/service/src/gen/thrift/gen-py/hive_service/ThriftHive-remote
@@ -29,6 +29,7 @@ if len(sys.argv) <= 1 or sys.argv[1] == '--help':
   print '  Schema getThriftSchema()'
   print '  HiveClusterStatus getClusterStatus()'
   print '  QueryPlan getQueryPlan()'
+  print '  void clean()'
   print ''
   sys.exit(0)
 
@@ -127,6 +128,12 @@ elif cmd == 'getQueryPlan':
     sys.exit(1)
   pp.pprint(client.getQueryPlan())
 
+elif cmd == 'clean':
+  if len(args) != 0:
+    print 'clean requires 0 args'
+    sys.exit(1)
+  pp.pprint(client.clean())
+
 else:
   print 'Unrecognized method %s' % cmd
   sys.exit(1)
diff --git a/service/src/gen/thrift/gen-py/hive_service/ThriftHive.py b/service/src/gen/thrift/gen-py/hive_service/ThriftHive.py
index 9277ced..1a16fcc 100644
--- a/service/src/gen/thrift/gen-py/hive_service/ThriftHive.py
+++ b/service/src/gen/thrift/gen-py/hive_service/ThriftHive.py
@@ -49,6 +49,9 @@ class Iface(hive_metastore.ThriftHiveMetastore.Iface):
   def getQueryPlan(self, ):
     pass
 
+  def clean(self, ):
+    pass
+
 
 class Client(hive_metastore.ThriftHiveMetastore.Client, Iface):
   def __init__(self, iprot, oprot=None):
@@ -278,6 +281,29 @@ class Client(hive_metastore.ThriftHiveMetastore.Client, Iface):
       raise result.ex
     raise TApplicationException(TApplicationException.MISSING_RESULT, "getQueryPlan failed: unknown result");
 
+  def clean(self, ):
+    self.send_clean()
+    self.recv_clean()
+
+  def send_clean(self, ):
+    self._oprot.writeMessageBegin('clean', TMessageType.CALL, self._seqid)
+    args = clean_args()
+    args.write(self._oprot)
+    self._oprot.writeMessageEnd()
+    self._oprot.trans.flush()
+
+  def recv_clean(self, ):
+    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
+    if mtype == TMessageType.EXCEPTION:
+      x = TApplicationException()
+      x.read(self._iprot)
+      self._iprot.readMessageEnd()
+      raise x
+    result = clean_result()
+    result.read(self._iprot)
+    self._iprot.readMessageEnd()
+    return
+
 
 class Processor(hive_metastore.ThriftHiveMetastore.Processor, Iface, TProcessor):
   def __init__(self, handler):
@@ -290,6 +316,7 @@ class Processor(hive_metastore.ThriftHiveMetastore.Processor, Iface, TProcessor)
     self._processMap["getThriftSchema"] = Processor.process_getThriftSchema
     self._processMap["getClusterStatus"] = Processor.process_getClusterStatus
     self._processMap["getQueryPlan"] = Processor.process_getQueryPlan
+    self._processMap["clean"] = Processor.process_clean
 
   def process(self, iprot, oprot):
     (name, type, seqid) = iprot.readMessageBegin()
@@ -418,6 +445,17 @@ class Processor(hive_metastore.ThriftHiveMetastore.Processor, Iface, TProcessor)
     oprot.writeMessageEnd()
     oprot.trans.flush()
 
+  def process_clean(self, seqid, iprot, oprot):
+    args = clean_args()
+    args.read(iprot)
+    iprot.readMessageEnd()
+    result = clean_result()
+    self._handler.clean()
+    oprot.writeMessageBegin("clean", TMessageType.REPLY, seqid)
+    result.write(oprot)
+    oprot.writeMessageEnd()
+    oprot.trans.flush()
+
 
 # HELPER FUNCTIONS AND STRUCTURES
 
@@ -1361,3 +1399,85 @@ class getQueryPlan_result:
 
   def __ne__(self, other):
     return not (self == other)
+
+class clean_args:
+
+  thrift_spec = (
+  )
+
+  def read(self, iprot):
+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
+      return
+    iprot.readStructBegin()
+    while True:
+      (fname, ftype, fid) = iprot.readFieldBegin()
+      if ftype == TType.STOP:
+        break
+      else:
+        iprot.skip(ftype)
+      iprot.readFieldEnd()
+    iprot.readStructEnd()
+
+  def write(self, oprot):
+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
+      return
+    oprot.writeStructBegin('clean_args')
+    oprot.writeFieldStop()
+    oprot.writeStructEnd()
+    def validate(self):
+      return
+
+
+  def __repr__(self):
+    L = ['%s=%r' % (key, value)
+      for key, value in self.__dict__.iteritems()]
+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))
+
+  def __eq__(self, other):
+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__
+
+  def __ne__(self, other):
+    return not (self == other)
+
+class clean_result:
+
+  thrift_spec = (
+  )
+
+  def read(self, iprot):
+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
+      return
+    iprot.readStructBegin()
+    while True:
+      (fname, ftype, fid) = iprot.readFieldBegin()
+      if ftype == TType.STOP:
+        break
+      else:
+        iprot.skip(ftype)
+      iprot.readFieldEnd()
+    iprot.readStructEnd()
+
+  def write(self, oprot):
+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
+      return
+    oprot.writeStructBegin('clean_result')
+    oprot.writeFieldStop()
+    oprot.writeStructEnd()
+    def validate(self):
+      return
+
+
+  def __repr__(self):
+    L = ['%s=%r' % (key, value)
+      for key, value in self.__dict__.iteritems()]
+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))
+
+  def __eq__(self, other):
+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__
+
+  def __ne__(self, other):
+    return not (self == other)
diff --git a/service/src/gen/thrift/gen-rb/thrift_hive.rb b/service/src/gen/thrift/gen-rb/thrift_hive.rb
index 926f643..29aa526 100644
--- a/service/src/gen/thrift/gen-rb/thrift_hive.rb
+++ b/service/src/gen/thrift/gen-rb/thrift_hive.rb
@@ -139,6 +139,20 @@ module ThriftHive
       raise ::Thrift::ApplicationException.new(::Thrift::ApplicationException::MISSING_RESULT, 'getQueryPlan failed: unknown result')
     end
 
+    def clean()
+      send_clean()
+      recv_clean()
+    end
+
+    def send_clean()
+      send_message('clean', Clean_args)
+    end
+
+    def recv_clean()
+      result = receive_message(Clean_result)
+      return
+    end
+
   end
 
   class Processor < ThriftHiveMetastore::Processor 
@@ -232,6 +246,13 @@ module ThriftHive
       write_result(result, oprot, 'getQueryPlan', seqid)
     end
 
+    def process_clean(seqid, iprot, oprot)
+      args = read_args(iprot, Clean_args)
+      result = Clean_result.new()
+      @handler.clean()
+      write_result(result, oprot, 'clean', seqid)
+    end
+
   end
 
   # HELPER FUNCTIONS AND STRUCTURES
@@ -500,5 +521,35 @@ module ThriftHive
     ::Thrift::Struct.generate_accessors self
   end
 
+  class Clean_args
+    include ::Thrift::Struct, ::Thrift::Struct_Union
+
+    FIELDS = {
+
+    }
+
+    def struct_fields; FIELDS; end
+
+    def validate
+    end
+
+    ::Thrift::Struct.generate_accessors self
+  end
+
+  class Clean_result
+    include ::Thrift::Struct, ::Thrift::Struct_Union
+
+    FIELDS = {
+
+    }
+
+    def struct_fields; FIELDS; end
+
+    def validate
+    end
+
+    ::Thrift::Struct.generate_accessors self
+  end
+
 end
 
diff --git a/service/src/java/org/apache/hadoop/hive/service/HiveServer.java b/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
index f2f2075..8f8ffa5 100644
--- a/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
+++ b/service/src/java/org/apache/hadoop/hive/service/HiveServer.java
@@ -18,11 +18,17 @@
 
 package org.apache.hadoop.hive.service;
 
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.FileReader;
 import java.io.IOException;
+import java.io.PrintStream;
+import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
 import java.util.List;
 
-import com.facebook.fb303.fb_status;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -48,11 +54,13 @@ import org.apache.thrift.transport.TServerTransport;
 import org.apache.thrift.transport.TTransport;
 import org.apache.thrift.transport.TTransportFactory;
 
+import com.facebook.fb303.fb_status;
+
 /**
  * Thrift Hive Server Implementation.
  */
 public class HiveServer extends ThriftHive {
-  private static final String VERSION = "0";
+  private static final String VERSION = "1";
 
   /**
    * Handler which implements the Hive Interface This class can be used in lieu
@@ -63,8 +71,14 @@ public class HiveServer extends ThriftHive {
     /**
      * Hive server uses org.apache.hadoop.hive.ql.Driver for run() and
      * getResults() methods.
+     * It is the instance of the last Hive query.
+     */
+    private Driver driver;
+    /**
+     * For processors other than Hive queries (Driver), they output to session.out (a temp file)
+     * first and the fetchOne/fetchN/fetchAll functions get the output from pipeIn.
      */
-    private final Driver driver;
+    private BufferedReader pipeIn;
 
     /**
      * Flag that indicates whether the last executed command was a Hive query.
@@ -80,12 +94,33 @@ public class HiveServer extends ThriftHive {
       super(HiveServer.class.getName());
 
       isHiveQuery = false;
+      driver = null;
       SessionState session = new SessionState(new HiveConf(SessionState.class));
       SessionState.start(session);
-      session.in = null;
-      session.out = null;
-      session.err = null;
-      driver = new Driver();
+      setupSessionIO(session);
+    }
+
+    private void setupSessionIO(SessionState session) {
+      try {
+        LOG.info("Putting temp output to file " + session.getTmpOutputFile().toString());
+        session.in = null; // hive server's session input stream is not used
+        // open a per-session file in auto-flush mode for writing temp results
+        session.out = new PrintStream(new FileOutputStream(session.getTmpOutputFile()), true, "UTF-8");
+        // TODO: for hadoop jobs, progress is printed out to session.err,
+        // we should find a way to feed back job progress to client
+        session.err = new PrintStream(System.err, true, "UTF-8");
+      } catch (IOException e) {
+        LOG.error("Error in creating temp output file ", e);
+        try {
+          session.in = null;
+          session.out = new PrintStream(System.out, true, "UTF-8");
+          session.err = new PrintStream(System.err, true, "UTF-8");
+	  } catch (UnsupportedEncodingException ee) {
+	    ee.printStackTrace();
+	    session.out = null;
+	    session.err = null;
+	  }
+      }
     }
 
     /**
@@ -96,7 +131,7 @@ public class HiveServer extends ThriftHive {
      */
     public void execute(String cmd) throws HiveServerException, TException {
       HiveServerHandler.LOG.info("Running the query: " + cmd);
-      SessionState.get();
+      SessionState session = SessionState.get();
 
       String cmd_trimmed = cmd.trim();
       String[] tokens = cmd_trimmed.split("\\s");
@@ -111,11 +146,14 @@ public class HiveServer extends ThriftHive {
         CommandProcessorResponse response = null;
         if (proc != null) {
           if (proc instanceof Driver) {
-            ((Driver)proc).destroy();
             isHiveQuery = true;
+            driver = (Driver) proc;
             response = driver.run(cmd);
           } else {
             isHiveQuery = false;
+            driver = null;
+            // need to reset output for each non-Hive query
+            setupSessionIO(session);
             response = proc.run(cmd_1);
           }
 
@@ -126,6 +164,7 @@ public class HiveServer extends ThriftHive {
       } catch (Exception e) {
         HiveServerException ex = new HiveServerException();
         ex.setMessage("Error running query: " + e.toString());
+        ex.setErrorCode(ret == 0? -10000: ret);
         throw ex;
       }
 
@@ -136,13 +175,32 @@ public class HiveServer extends ThriftHive {
     }
 
     /**
+     * Should be called by the client at the end of a session.
+     */
+    public void clean() {
+      if (driver != null) {
+        driver.close();
+        driver.destroy();
+      }
+
+      SessionState session = SessionState.get();
+      if (session.getTmpOutputFile() != null) {
+        session.getTmpOutputFile().delete();
+      }
+      pipeIn = null;
+    }
+
+    /**
      * Return the status information about the Map-Reduce cluster.
      */
     public HiveClusterStatus getClusterStatus() throws HiveServerException,
         TException {
       HiveClusterStatus hcs;
       try {
-        ClusterStatus cs = driver.getClusterStatus();
+        Driver drv = new Driver();
+        drv.init();
+
+        ClusterStatus cs = drv.getClusterStatus();
         JobTracker.State jbs = cs.getJobTrackerState();
 
         // Convert the ClusterStatus to its Thrift equivalent: HiveClusterStatus
@@ -181,6 +239,8 @@ public class HiveServer extends ThriftHive {
         return new Schema();
       }
 
+      assert driver != null: "getSchema() is called on a Hive query and driver is NULL.";
+
       try {
         Schema schema = driver.getSchema();
         if (schema == null) {
@@ -206,6 +266,8 @@ public class HiveServer extends ThriftHive {
         return new Schema();
       }
 
+      assert driver != null: "getThriftSchema() is called on a Hive query and driver is NULL.";
+
       try {
         Schema schema = driver.getThriftSchema();
         if (schema == null) {
@@ -222,6 +284,7 @@ public class HiveServer extends ThriftHive {
       }
     }
 
+
     /**
      * Fetches the next row in a query result set.
      *
@@ -231,9 +294,17 @@ public class HiveServer extends ThriftHive {
     public String fetchOne() throws HiveServerException, TException {
       if (!isHiveQuery) {
         // Return no results if the last command was not a Hive query
-        return "";
+        List<String> results = new ArrayList<String>(1);
+        readResults(results, 1);
+        if (results.size() > 0) {
+          return results.get(0);
+        } else { //  throw an EOF exception
+          throw new HiveServerException("OK", 0, "");
+        }
       }
 
+      assert driver != null: "fetchOne() is called on a Hive query and driver is NULL.";
+
       ArrayList<String> result = new ArrayList<String>();
       driver.setMaxRows(1);
       try {
@@ -243,7 +314,8 @@ public class HiveServer extends ThriftHive {
         // TODO: Cannot return null here because thrift cannot handle nulls
         // TODO: Returning empty string for now. Need to figure out how to
         // TODO: return null in some other way
-        return "";
+        throw new HiveServerException("OK", 0, "");
+       // return "";
       } catch (IOException e) {
         HiveServerException ex = new HiveServerException();
         ex.setMessage(e.getMessage());
@@ -251,6 +323,56 @@ public class HiveServer extends ThriftHive {
       }
     }
 
+    private void cleanTmpFile() {
+      if (pipeIn != null) {
+        SessionState session = SessionState.get();
+        File tmp = session.getTmpOutputFile();
+        tmp.delete();
+        pipeIn = null;
+      }
+    }
+
+    /**
+     * Reads the temporary results for non-Hive (non-Driver) commands to the
+     * resulting List of strings.
+     * @param results list of strings containing the results
+     * @param nLines number of lines read at once. If it is <= 0, then read all lines.
+     */
+    private void readResults(List<String> results, int nLines) {
+
+      if (pipeIn == null) {
+        SessionState session = SessionState.get();
+        File tmp = session.getTmpOutputFile();
+        try {
+          pipeIn = new BufferedReader(new FileReader(tmp));
+        } catch (FileNotFoundException e) {
+          LOG.error("File " + tmp + " not found. ", e);
+          return;
+        }
+      }
+
+      boolean readAll = false;
+
+      for (int i = 0; i < nLines || nLines <= 0; ++i) {
+        try {
+          String line = pipeIn.readLine();
+          if (line == null) {
+            // reached the end of the result file
+            readAll = true;
+            break;
+          } else {
+            results.add(line);
+          }
+        } catch (IOException e) {
+          LOG.error("Reading temp results encountered an exception: ", e);
+          readAll = true;
+        }
+      }
+      if (readAll) {
+        cleanTmpFile();
+      }
+    }
+
     /**
      * Fetches numRows rows.
      *
@@ -270,12 +392,16 @@ public class HiveServer extends ThriftHive {
         ex.setMessage("Invalid argument for number of rows: " + numRows);
         throw ex;
       }
+
+      ArrayList<String> result = new ArrayList<String>();
+
       if (!isHiveQuery) {
-        // Return no results if the last command was not a Hive query
-        return new ArrayList<String>();
+        readResults(result, numRows);
+        return result;
       }
 
-      ArrayList<String> result = new ArrayList<String>();
+      assert driver != null: "fetchN() is called on a Hive query and driver is NULL.";
+
       driver.setMaxRows(numRows);
       try {
         driver.getResults(result);
@@ -298,13 +424,16 @@ public class HiveServer extends ThriftHive {
      *         in the client.
      */
     public List<String> fetchAll() throws HiveServerException, TException {
-      if (!isHiveQuery) {
-        // Return no results if the last command was not a Hive query
-        return new ArrayList<String>();
-      }
 
       ArrayList<String> rows = new ArrayList<String>();
       ArrayList<String> result = new ArrayList<String>();
+
+      if (!isHiveQuery) {
+        // Return all results if numRows <= 0
+        readResults(result, 0);
+        return result;
+      }
+
       try {
         while (driver.getResults(result)) {
           rows.addAll(result);
@@ -337,6 +466,13 @@ public class HiveServer extends ThriftHive {
     @Override
     public QueryPlan getQueryPlan() throws HiveServerException, TException {
       QueryPlan qp = new QueryPlan();
+
+      if (!isHiveQuery) {
+        return qp;
+      }
+
+      assert driver != null: "getQueryPlan() is called on a Hive query and driver is NULL.";
+
       // TODO for now only return one query at a time
       // going forward, all queries associated with a single statement
       // will be returned in a single QueryPlan
@@ -378,12 +514,17 @@ public class HiveServer extends ThriftHive {
       SessionState.initHiveLog4j();
 
       int port = 10000;
+      int minWorkerThreads = 100; // default number of threads serving the Hive server
       if (args.length >= 1) {
         port = Integer.parseInt(args[0]);
       }
+      if (args.length >= 2) {
+        minWorkerThreads = Integer.parseInt(args[1]);
+      }
       TServerTransport serverTransport = new TServerSocket(port);
       ThriftHiveProcessorFactory hfactory = new ThriftHiveProcessorFactory(null);
       TThreadPoolServer.Options options = new TThreadPoolServer.Options();
+      options.minWorkerThreads = minWorkerThreads;
       TServer server = new TThreadPoolServer(hfactory, serverTransport,
           new TTransportFactory(), new TTransportFactory(),
           new TBinaryProtocol.Factory(), new TBinaryProtocol.Factory(), options);
diff --git a/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java b/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
index ee6be7c..e92ece1 100644
--- a/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
+++ b/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java
@@ -162,7 +162,12 @@ public class TestHiveServer extends TestCase {
     assertEquals(thriftschema.getFieldSchemasSize(), 0);
     assertEquals(thriftschema.getPropertiesSize(), 0);
 
-    assertEquals(client.fetchOne(), "");
+    try {
+      String ret = client.fetchOne();
+      assertTrue(false);
+    } catch (HiveServerException e) {
+      assertEquals(e.getErrorCode(), 0);
+    }
     assertEquals(client.fetchN(10).size(), 0);
     assertEquals(client.fetchAll().size(), 0);
 
@@ -181,7 +186,12 @@ public class TestHiveServer extends TestCase {
     assertEquals(thriftschema.getFieldSchemasSize(), 0);
     assertEquals(thriftschema.getPropertiesSize(), 0);
 
-    assertEquals(client.fetchOne(), "");
+    try {
+      String ret = client.fetchOne();
+      assertTrue(false);
+    } catch (HiveServerException e) {
+      assertEquals(e.getErrorCode(), 0);
+    }
     assertEquals(client.fetchN(10).size(), 0);
     assertEquals(client.fetchAll().size(), 0);
 
@@ -243,12 +253,17 @@ public class TestHiveServer extends TestCase {
       // fetchOne test
       client.execute("select key, value from " + tableName);
       for (int i = 0; i < 500; i++) {
-        String str = client.fetchOne();
-        if (str.equals("")) {
+        try {
+          String str = client.fetchOne();
+        } catch (HiveServerException e) {
           assertTrue(false);
         }
       }
-      assertEquals(client.fetchOne(), "");
+      try {
+        client.fetchOne();
+      } catch (HiveServerException e) {
+        assertEquals(e.getErrorCode(), 0);
+      }
 
       // fetchN test
       client.execute("select key, value from " + tableName);
-- 
1.7.0.4

