
\section{Related Work}

Conceptually, Shark's operators are standard relational operators and we employ standard traditional techniques, e.g. predicate pushdowns, in distributed databases for query optimization and processing \cite{distributed-db-ozsu}. The work, however, is also inspired by a number of projects from the database community as well as the systems community.

\textbf{Massively Parallel Databases}: Since Jeff Dean et al proposed the Google MapReduce infrastructure in 2004 \cite{mapreduce} and demonstrated extreme scalability and flexibility with the system, a number of recent projects from both the industry and the academia have focused on fitting declarative data processing onto the MapReduce style computation framework. \cite{hive, pig, tenzing, hyracks, asterix} focus mostly on providing to end users a higher level declarative interface that is compiled down to MapReduce tasks. Greenplum and Aster Data have added the ability to execute MapReduce-style functions over data stored in these systems. \cite{hadoopdb, split-execution} explore on the architectural level exploiting hybrid MapReduce and relational database systems. Dremel, another project from Google \cite{dremel}, is worth mentioning as an example of a new generation of database systems that are massively distributed and run interactive queries on very large data sets. Shark differs from these projects mainly in its novel use of RDDs for caching data.

\textbf{Distributed In-Memory Computation}: A number of researchers are now focusing on in-memory solutions for cluster computing and data management. The RAMCloud project at Stanford \cite{ramcloud} aims at creating a low latency distributed hash table for a wide range of applications. RAMCloud exposes the ability to do fine-grained updates, and the entire system's write throughput is limited by the aggregated write throughput of all disks. MIT's H-Store \cite{hstore} is a distributed main memory database. It features an architecture similar to traditional relational databases, albeit optimized for memory operations. H-Store focuses mainly in transaction processing and provides ACID guarantees. In contrary, Shark is designed from ground-up to exploit RDD for data warehousing.
% maybe mention piccolo?

\textbf{Distributed Machine Learning}: One design goal of Shark is to unify data exploration in SQL and sophisticated data analyses in one system. A number of changes have been suggested to improve the standard MapReduce framework that can significantly benefit iterative machine learning jobs. For example, the HaLoop project at the University of Washington \cite{haloop} enables the specification of cyclic workflows, while the MapReduce Online project at Berkeley \cite{mapreduce-online} allows data to be piped between operators to reduce the latency of iterative jobs.

The machine learning community has been investigating what algorithms fit more naturally into this programming paradigm. \cite{MapReduceML} analyzed ten different machine learning algorithms and pointed out that 10 common machine learning algorithms belong to a family called Summation Form algorithms and can be executed in an ``embarrassingly parallel'' fashion using MapReduce. In \cite{Gillick}, Gillick proposes a taxonomy of standard machine learning algorithms: single-pass learning, iterative learning, and query-based learning with distance metrics. He then analyzes the performance characteristics of these algorithms implemented in MapReduce. Gillick also discusses the complications of using MapReduce to implement more advanced machine learning algorithms and proposes improvements, primarily shared-state among mappers and static typing, to Hadoop to ease these problems.

